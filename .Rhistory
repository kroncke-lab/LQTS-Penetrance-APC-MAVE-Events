library(flexsurv)
library(survivalROC)
source('/Users/KRONCKE/OneDrive - VUMC/Kroncke_Lab/GitHub/Bayes_KCNH2_LQT2_Penetrance/func_dist_seq.R')
#Brier Score
brierScore <- function(data,prediction,num_positive,num_negative){
sum(data[,num_positive]*(data[,prediction]-1)^2 +
data[,num_negative]*(data[,prediction]-0)^2)/sum(data[,num_negative]+data[,num_positive])
}
#Reference Brier Score
brierScore_ref <- function(data, a_0, b_0,num_positive,num_negative){
sum(data[,num_positive]*((a_0/(a_0+b_0))-1)^2 +
data[,num_negative]*((a_0/(a_0+b_0))-0)^2)/sum(data[,num_negative]+data[,num_positive])
}
#Alternative Reference Brier Score
brierScore_alt_ref <- function(data, num_positive, num_negative){
sum(data[,num_positive]*(alt_mean-1)^2 +
data[,num_negative]*(alt_mean-0)^2)/sum(data[,num_negative]+data[,num_positive])
}
#Binary Cross-Entropy/Log Loss
BCE <- function(data, prediction, Truth){
-sum(data[,Truth]*log(data[,prediction]) + (1-data[,Truth])*log(1-data[,prediction]))/length(data[,prediction])
}
#Binary Cross-Entropy/Log Loss
KL_div <- function(data, prediction, Truth){
-sum(data[,Truth]*log(data[,Truth]/data[,prediction]))
}
#Net Benefit
netbenefit <- function(data, th, predictor, num_positive, num_total){
sum(data[data[,predictor]>th, num_positive])/sum(data[, num_total]) -
((sum(data[data[,predictor]>th, num_total])-sum(data[data[,predictor]>th, num_positive]))/
sum(data[, num_total]))*(th/(1-th))
}
#Net Benefit Previous
netbenefit_fig <- function(data, th, predictor, num_positive, num_total){
nb<-data.frame()
i <- 0
for (i in seq(0,100,1)){
nb[i+1,predictor] <- sum(data[data[,predictor]>i/250, num_positive])/sum(data[, num_total])  -
((sum(data[data[,predictor]>i/250, num_total])-sum(data[data[,predictor]>i/250, num_positive]))/
sum(data[, num_total]))*(i/250/(1-i/250))
nb[i+1,"threshold"] <- i/250
}
return(nb)
}
calcPval=function(xName,yName,weightName,nPerms,new.mat2){
# Pulls out variables
x=new.mat2[,xName]
y=new.mat2[,yName]
w=new.mat2[,weightName]
x2=x[!is.na(x)]
y2=y[!is.na(x)]
w2=w[!is.na(x)]
# Calculate the real correlation
realCorr=weightedCorr(x2,y2,method='spearman',weights=w2)
# Do permutations, calculate fake correlations
permutedCorrList=c()
for(permNum in 1:nPerms){
permutedX=sample(x2,length(x2),replace=FALSE)
wCorrSim=weightedCorr(permutedX,y2,method='spearman',weights=w2)
permutedCorrList=c(permutedCorrList,wCorrSim)
}
permutedCorrList2=abs(permutedCorrList)
realCorr2=abs(realCorr)
# Calculate pvalue
summ=sum(realCorr2<permutedCorrList2)
pValue=summ/nPerms
return(list(realCorr,pValue,length(x2)))
}
calcAllPvals=function(yList,xList,nPerms,weightName,new.mat2){
i=0
resultTable=data.frame()
for(yName in yList){
for(xName in xList){
i=i+1
result=calcPval(xName,yName,weightName,nPerms,new.mat2)
resultTable[i,'x']=xName
resultTable[i,'y']=yName
resultTable[i,'nPerms']=nPerms
resultTable[i,'weightedCorr']=result[[1]]
resultTable[i,'pValue']=result[[2]]
resultTable[i,'n']=result[[3]]
#print(resultTable[i,'pValue'])
}
}
print(resultTable)
return(resultTable)
}
# Mean squared error
mse <- function(sm) {
mean((sm$residuals)^2*(sm$weights))
}
# Derive alpha and beta from weighted mean and MSE (estimated variance)
estBetaParams <- function(mu, var) {
alpha <- ((1 - mu) / var - 1 / mu) * mu ^ 2
beta <- alpha * (1 / mu - 1)
return(params = list(alpha = alpha, beta = beta))
}
# solve for alpha and beta in Beta distribution
solab <- function(mean, variance) {
alpha <- (mean^2 * (1 - mean) - variance * mean) / variance
beta <- alpha * (1 / mean - 1)
return(matrix(c(alpha, beta), ncol = 1, byrow = TRUE))
}
# Load distance matrix
h2dist<-read.csv(file = "/Users/KRONCKE/OneDrive - VUMC/Kroncke_Lab/GitHub/Bayes_KCNH2_LQT2_Penetrance/5va1.dists.txt", header = FALSE)
#Load in previous dataset from Variantbrowser as 'd'
con = dbConnect(SQLite(), dbname="/Users/KRONCKE/OneDrive - VUMC/Kroncke_Lab/kcnh2/VariantDatabase/VariantKCNH2-v4_w_paris_japan_mayo_and_italycohort.db")
alltables = dbListTables(con)
my.data <- dbReadTable(con, 'VariantKCNH2')
my.data[my.data=='NA'] <- NA
e<-my.data
dbDisconnect(con)
#d<-read.csv("/Users/KRONCKE/OneDrive - VUMC/Kroncke_Lab/GitHub/Bayes_KCNH2_LQT2_Penetrance/variant_browser/all_vars_annotated.csv")
#e<-d
e<-e[,c("var","lqt2","unaff","gnomAD")]
load("C:/Users/KRONCKE/OneDrive - VUMC/Kroncke_Lab/GitHub/Bayes_KCNH2_LQT2_Penetrance/lit_all_data_checkpoint.RData")
d<-d[,c("var", "isoform", "mut_type", "pph2_prob", "blast_pssm", "provean_score", "pamscore", "aasimilaritymat", "revel_score", "cardiacboost")]
d<-merge(d,e,all = T)
#Import new gnomAD
gnomad.v4<-read.csv('/Users/KRONCKE/Dropbox/sat paper/Modelling/gnomad_v4_simple.csv')
gnomad.v4<-unique(gnomad.v4)
gnomad.v4<-gnomad.v4[,c("var","gnomad.v4")]
#load clinvar
clinvar <- read.csv('/Users/KRONCKE/Dropbox/sat paper/Modelling/clinvar_result_kcnh2_20240222.csv')
clinvar<-unique(clinvar)
clinvar<-clinvar[,c("var","clinvar","clinvar.description")]
#Load first_event_data.cohort
first_event_data.cohort<-read.csv("/Users/KRONCKE/OneDrive - VUMC/Kroncke_Lab/kcnh2/VariantDatabase/Network_KCNH2_variants/first.event.cohort.csv")
first_serious_event_data.cohort<-read.csv("/Users/KRONCKE/OneDrive - VUMC/Kroncke_Lab/kcnh2/VariantDatabase/Network_KCNH2_variants/first.serious.event.cohort.csv")
cohort<-read.csv("/Users/KRONCKE/OneDrive - VUMC/Kroncke_Lab/kcnh2/VariantDatabase/Network_KCNH2_variants/KCNH2.recurrent.events.cohort.csv")
cut_time <- 20
# Define the function to modify and possibly split records
modify_split_records <- function(record) {
# Initialize a list to store records
records <- list()
# Modify time_sex based on Age_at_event_end
record$time_sex <- ifelse(record$Age_at_event_end < cut_time, 0, 1)
# Check if a new record needs to be added
if (record$Age_at_event_start < cut_time && record$Age_at_event_end > cut_time) {
# Create a new record with adjusted values
new_record <- record
new_record$Age_at_event_start <- cut_time
new_record$time_sex <- 1
# Modify the original record
record$Age_at_event_end <- cut_time
record$time_sex <- 0
record$Event <- 0
# Add both records to the list
records[[1]] <- record
records[[2]] <- new_record
} else {
# Add the single modified record to the list
records[[1]] <- record
}
# Combine the records in the list into a single dataframe
do.call(rbind, records)
}
# Apply the function to each row of the dataset
cohort_modified <- do.call(rbind, lapply(split(cohort, rownames(cohort)), modify_split_records))
# Arrange the data frame by Unique_ID and Age_at_event_start
cohort_combined <- arrange(cohort_modified, Unique_ID, Age_at_event_start)
cohort <- cohort_combined
# Convert 'Sex' to numeric: Female = 1, Male = 0 (or another value)
cohort$time.sex <- ifelse(cohort$Sex == "Female", 1, 0) * cohort$time_sex
# load trafficking data
traff<-read.csv("/Users/KRONCKE/Dropbox/sat paper/Functional Data/trafficking-whole-enchilada.csv")
# Step 1 and 2: Group by 'resnum' and calculate the proportion of missing values in 'traff_score'
traff_grouped <- traff %>%
group_by(resnum) %>%
summarise(prop_missing = mean(is.na(score)))
# Step 3: Filter out groups with more than 50% missing values
resnums_to_keep <- traff_grouped %>%
filter(prop_missing < 0.25) %>%
pull(resnum)
# Step 4: Filter the original dataframe to keep only the selected 'resnum'
filtered_traff <- traff %>%
filter(resnum %in% resnums_to_keep)
traff<-filtered_traff[,c("score","score_SE","var")]
colnames(traff)[1] <- "traff_score"
traff$traff_score <- traff$traff_score/100 #(-1)*(traff$traff_score-100)/100
traff[!is.na(traff$traff_score) & traff$traff_score<0,"traff_score"] <- 0
traff[!is.na(traff$traff_score) & traff$traff_score>1,"traff_score"] <- 1
traff <- traff[traff$score_SE<20,]
traff$traff_score <- (traff$traff_score*(-1) + 1)*4
#load alphamissense
alpha.miss<-read.csv('/Users/KRONCKE/Dropbox/sat paper/Functional Data/Q12809.alpha.missense.txt', sep = "")
names(alpha.miss)[2]<-"var"
alpha.miss<-alpha.miss[,c('var',"am_pathogenicity")]
#Varity variant function prediction (Roth Lab)
varity<-read.csv("/Users/KRONCKE/Dropbox/sat paper/Modelling/KCNH2[Q12809]_1-1000_VARITY_R_20230207192328620530.csv")
varity<-varity[!is.na(varity$VARITY_R), c( "aa_pos", "aa_ref", "aa_alt", "VARITY_R", "VARITY_R_LOO", "VARITY_ER", "VARITY_ER_LOO")]
names(varity)[1:3]<-c("resnum", "wtAA", "mutAA")
varity$var <- paste(varity$wtAA,varity$resnum,varity$mutAA, sep = "")
#load EVE dataset (in silico predictor)
eve<-read.csv("/Users/KRONCKE/OneDrive - VUMC/Kroncke_Lab/kcnh2/VariantDatabase/PredictiveFeatureServers/KCNH2_HUMAN_EVE.csv")
#Load peak tail current
sync<-read.csv('/Users/KRONCKE/Dropbox/sat paper/Functional Data/533variants_CD.csv')
sync<-sync[,c("var", "peak_tail")]
sync_2<-read.csv('/Users/KRONCKE/Dropbox/sat paper/AJHG_published_paper/peak_tail_50mV.csv')
sync_2<-sync_2[,c("var", "peak_tail")]
sync_2$peak_tail<-sync_2$peak_tail*100
sync_3<-read.csv('/Users/KRONCKE/Dropbox/sat paper/gnomAD variants_200123.csv')
sync_3$peak_tail<-sync_3$peak_tail*100
# removing duplicate peak tail current measurements with preference towards sync>sync_2>sync_3>sync_4
sync_names<-unique(sync$var)
sync_2 <- sync_2[!sync_2$var %in% sync_names, ]
sync_2 <- unique(sync_2)
test2<-merge(sync,sync_2, all = T)
sync_names<-unique(test2$var)
sync_3 <- sync_3[!sync_3$var %in% sync_names, ]
sync_3 <- unique(sync_3)
test<-merge(test2,sync_3, all = T)
sync_names<-unique(test$var)# test for comparison with 'test'
# Rescale peak tail
test$peak_tail <- test$peak_tail/100  #(-1)*(test$peak_tail-100)/100
test[!is.na(test$peak_tail) & test$peak_tail<0,"peak_tail"] <- 0
test[!is.na(test$peak_tail) & test$peak_tail>1,"peak_tail"] <- 1
test[!is.na(test$peak_tail),"peak_tail"] <- (test[!is.na(test$peak_tail),"peak_tail"]*(-1) + 1)*4
#merge Splice AI and Varity then rename back to 'd'
e<-merge(d,varity, all = T)
g<-merge(e,alpha.miss, all = T)
f<-merge(g,clinvar, all = T)
h<-merge(f,gnomad.v4, all = T)
d<-h
#Merge
e<-merge(d,test, all = T)
f<-merge(e,traff, all = T)
cohort.vars <- as.data.frame(unique(first_event_data.cohort$var))
names(cohort.vars)<-'var'
cohort.vars$cohort<-1
g<-merge(f,cohort.vars,all = T)
g[is.na(g$cohort),"cohort"]<-0
f<-g
# Assuming f is your dataframe and f$var contains the strings
pattern <- "([A-Z])|([0-9]+)|(fsX)|(Del)|(Ins)"
matches <- regmatches(f$var, gregexpr(pattern, f$var))
# Extract the second part (the numbers)
f$resnum <- sapply(matches, function(x) x[2])
# Extract the first part (the first match, typically an uppercase letter)
f$wtAA <- sapply(matches, function(x) ifelse(length(x) > 0, x[1], NA))
# Extract the last part (the last match, either an uppercase letter, 'fsX', 'Del', or 'Ins')
f$mutAA <- sapply(matches, function(x) {
if (length(x) > 0) {
last_match <- x[length(x)]
if (last_match %in% c("Del", "Ins")) {
return(last_match)
} else {
return(x[length(x)])
}
} else {
return(NA)
}
})
# Logical vectors indicating whether each element contains a number
wtAA_has_number <- grepl("[0-9]", f$wtAA)
mutAA_has_number <- grepl("[0-9]", f$mutAA)
resnum_has_letter <- grepl("[A-Z]", f$resnum)
# Subset the dataframe to exclude rows where either column contains a number
f_filtered <- f[!(wtAA_has_number | mutAA_has_number | resnum_has_letter), ]
f_filtered<-f_filtered[f_filtered$wtAA != f_filtered$mutAA,]
f<-f_filtered
f<-f[f$mutAA !='fsX' & f$mutAA !='X' & !is.na(f$wtAA),] # & f$total_carriers>0
f<-f[f$isoform != "B" & f$isoform != "C" & f$mut_type != "nonsense",]
f<-f[!is.na(f$var),]
# remove duplicates from degenerate SNVs
variants<-data.frame(var=unique(f$var))
red_merge<-data.frame()
for (i in 1:length(variants$var)){
red_merge<-rbind(red_merge,f[match(variants[i,"var"],f$var),])
}
f<-red_merge
#calculate lqt2 penetrance
f[is.na(f$gnomad.v4),"gnomad.v4"]<-0
f[is.na(f$gnomAD),"gnomAD"]<-0
f$gnomAD<-as.integer(f$gnomAD)
f[is.na(f$lqt2),"lqt2"]<-0
f[is.na(f$unaff),"unaff"]<-0
#f[is.na(f$clinvar),"clinvar"]<-0
f$total_carriers<-f$lqt2 + f$unaff + f$gnomad.v4
f[is.na(f$total_carriers),"total_carriers"]<-0
f$penetrance_lqt2<-f$lqt2/f$total_carriers
f$weight = 1-1/(0.01+f$total_carriers)
f[f$total_carriers < 1,"weight"] <- 0.000
#lable pore region
f$pore <- 0
f$pore[f$resnum>550 & f$resnum<651] <- 1
# True lqt2_dist for evaluation of Bayesian posterior
f[, "lqt2_dist"]<-NA
f[, "lqt2_dist_weight"]<-NA
ld<-0
for(rec in seq(2,1159,1)){
print(rec)
ld <- funcdist(rec, "var", f[!is.na(f$total_carriers) & f$total_carriers>0,], h2dist, "penetrance_lqt2", "sigmoid", 7)
f[f$resnum == rec,"lqt2_dist"] <- ld[1]
f[f$resnum == rec,"lqt2_dist_weight"] <-ld[2]
}
# remove NAs from f$penetrance_lqt2, and remove extreme high/low penetrance
f[is.nan(f$penetrance_lqt2),"penetrance_lqt2"]<-0.0005
f[is.nan(f$penetrance_lqt2),"weight"]<-0
f[f$penetrance_lqt2==0,"penetrance_lqt2"]<-0.0005
f[f$penetrance_lqt2==1,"penetrance_lqt2"]<-0.9995
# Weighted mean to determine LQT2 penetrance empirical prior
newdata = data.frame(wt=1)
model <- lm(penetrance_lqt2 ~ 1, data=f, weights = f$weight)
summary(model)
p<-predict(model, newdata)
dev<-mse(model)#p*(1-p)
#model <- lm(penetrance_lqt2 ~ 1, data=f, weights = f$total_carriers)
#alt_mean<-model$fitted.values[1]
# Estimated shape parameters for LQT2 empirical prior
alpha0 = estBetaParams(p,dev)$alpha
beta0 = estBetaParams(p,dev)$beta
print(paste("alpha0 = ", alpha0, "  beta0 = ", beta0))
# Bayesian LQT2 penetrance estimates from empirical priors
# and observed affected/unaffected counts:
f$lqt2_penetranceBayesian_initial <- (alpha0 + f[,"lqt2"])/((alpha0 + beta0 + f[,"total_carriers"]))
f$lqt2_penetranceBayesian<-f$lqt2_penetranceBayesian_initial
# To reduce calculation time, remove all variants without carrier observations
f$tot.cohort<-f$total_carriers+f$cohort
#f<-f[f$tot.cohort>0,]
f<-f[!is.na(f$var),]
# Initializing Features
f$p_mean_w <- f$lqt2_penetranceBayesian_initial
covariates <- c("traff_score", "lqt2_dist", "am_pathogenicity", "pore") #
complete_rows_in_f <- complete.cases(f[, covariates])
tmp<-f[complete_rows_in_f,]
# Initialize variables
delta <- 10
count <- 0
# EM Loop
while(delta > 0.01 & count < 20) {
count <- count + 1
# Fit the model using 'f' with complete data for current covariates
regression_formula <- as.formula(paste("p_mean_w ~", paste(covariates, collapse = " + ")))
model <- glm(regression_formula, data = tmp, weights = tmp$weight)
# Make predictions and update only those rows in 'tmp'
prediction_output <- predict(model, newdata = tmp, se.fit = TRUE, type = "response")
predictions <- pmin(pmax(prediction_output$fit, 0.0005), 1)
variance_f <- (prediction_output$se.fit)^2
# Assuming predictions and variance_f are vectors
alpha_beta <- solab(predictions, variance_f)
# Length of each segment (assuming alpha_beta is a single column)
segment_length <- length(alpha_beta) / 2
# Extract alpha values (first half of alpha_beta)
alpha_f <- alpha_beta[1:segment_length]
beta_f <- alpha_beta[(segment_length + 1):(2 * segment_length)]
new_mean <- (alpha_f + tmp$lqt2) / (alpha_f + beta_f + tmp$total_carriers)
delta <- 100 * sum(abs(new_mean - tmp$p_mean_w)) / length(tmp$var)
tmp$p_mean_w <- new_mean
tmp[tmp$p_mean_w<0,"p_mean_w"]<-0.005
mean_alpha <- mean(alpha_f)
mean_beta <- mean(beta_f)
print(paste("Mean alpha:", mean_alpha, "Mean beta:", mean_beta))
print(paste("Delta:", delta, "Count:", count))
}
# when tuning parameter is 11, predictions are equivalent to "nu" variant heterozygote phenotypes
nu <- 10
prior_mean <- tmp$p_mean_w
tmp$prior_mean <- tmp$p_mean_w
variance <- prior_mean*(1-prior_mean)
variance <- variance / (1 + nu)
ind_a <- seq(1, length(variance),1)
ind_b <- seq(length(variance)+1, length(variance)*2,1)
alpha <- solab(prior_mean,variance)[ind_a]
beta <- solab(prior_mean,variance)[ind_b]
new_mean <- (alpha + tmp$lqt2)/(alpha + beta + tmp$total_carriers)
tmp$p_mean_w <- new_mean
tmp$alpha <- alpha
tmp$beta <- beta
sub.tmp<-tmp # reassign tmp so I can still use it in the next chunk
covariates <- c("lqt2_dist", "am_pathogenicity", "pore")
new_complete_rows_in_f <- complete.cases(f[, covariates])
tmp<-f[new_complete_rows_in_f,]
delta <- 10
count <- 0
# EM Loop
while(delta > 0.01 & count < 20) {
count <- count + 1
# Fit the model using 'f' with complete data for current covariates
regression_formula <- as.formula(paste("p_mean_w ~", paste(covariates, collapse = " + ")))
model <- glm(regression_formula, data = tmp, weights = tmp$weight)
# Make predictions and update only those rows in 'tmp'
prediction_output <- predict(model, newdata = tmp, se.fit = TRUE, type = "response")
predictions <- pmin(pmax(prediction_output$fit, 0.0005), 1)
variance_f <- (prediction_output$se.fit)^2
# Assuming predictions and variance_f are vectors
alpha_beta <- solab(predictions, variance_f)
# Length of each segment (assuming alpha_beta is a single column)
segment_length <- length(alpha_beta) / 2
# Extract alpha values (first half of alpha_beta)
alpha_f <- alpha_beta[1:segment_length]
beta_f <- alpha_beta[(segment_length + 1):(2 * segment_length)]
new_mean <- (alpha_f + tmp$lqt2) / (alpha_f + beta_f + tmp$total_carriers)
delta <- 100 * sum(abs(new_mean - tmp$p_mean_w)) / length(tmp$var)
tmp$p_mean_w <- new_mean
tmp[tmp$p_mean_w<0,"p_mean_w"]<-0.005
mean_alpha <- mean(alpha_f)
mean_beta <- mean(beta_f)
print(paste("Mean alpha:", mean_alpha, "Mean beta:", mean_beta))
print(paste("Delta:", delta, "Count:", count))
}
# when tuning parameter is 11, predictions are equivalent to 10 variant heterozygote phenotypes
prior_mean <- tmp$p_mean_w
tmp$prior_mean <- tmp$p_mean_w
variance <- prior_mean*(1-prior_mean)
variance <- variance / (1 + nu)
ind_a <- seq(1, length(variance),1)
ind_b <- seq(length(variance)+1, length(variance)*2,1)
alpha <- solab(prior_mean,variance)[ind_a]
beta <- solab(prior_mean,variance)[ind_b]
new_mean <- (alpha + tmp$lqt2)/(alpha + beta + tmp$total_carriers)
tmp$p_mean_w <- new_mean
tmp$alpha <- alpha
tmp$beta <- beta
sub.tmp1 <- subset(tmp, !var %in% sub.tmp$var)
covariates <- c("lqt2_dist", "pore")
new_complete_rows_in_f <- complete.cases(f[, covariates])
tmp<-f[new_complete_rows_in_f,]
delta <- 10
count <- 0
# EM Loop
while(delta > 0.01 & count < 20) {
count <- count + 1
# Fit the model using 'f' with complete data for current covariates
regression_formula <- as.formula(paste("p_mean_w ~", paste(covariates, collapse = " + ")))
model <- glm(regression_formula, data = tmp, weights = tmp$weight)
# Make predictions and update only those rows in 'tmp'
prediction_output <- predict(model, newdata = tmp, se.fit = TRUE, type = "response")
predictions <- pmin(pmax(prediction_output$fit, 0.0005), 1)
variance_f <- (prediction_output$se.fit)^2
# Assuming predictions and variance_f are vectors
alpha_beta <- solab(predictions, variance_f)
# Length of each segment (assuming alpha_beta is a single column)
segment_length <- length(alpha_beta) / 2
# Extract alpha values (first half of alpha_beta)
alpha_f <- alpha_beta[1:segment_length]
beta_f <- alpha_beta[(segment_length + 1):(2 * segment_length)]
new_mean <- (alpha_f + tmp$lqt2) / (alpha_f + beta_f + tmp$total_carriers)
delta <- 100 * sum(abs(new_mean - tmp$p_mean_w)) / length(tmp$var)
tmp$p_mean_w <- new_mean
tmp[tmp$p_mean_w<0,"p_mean_w"]<-0.005
mean_alpha <- mean(alpha_f)
mean_beta <- mean(beta_f)
print(paste("Mean alpha:", mean_alpha, "Mean beta:", mean_beta))
print(paste("Delta:", delta, "Count:", count))
}
# when tuning parameter is 11, predictions are equivalent to 10 variant heterozygote phenotypes
prior_mean <- tmp$p_mean_w
tmp$prior_mean <- tmp$p_mean_w
variance <- prior_mean*(1-prior_mean)
variance <- variance / (1 + nu)
ind_a <- seq(1, length(variance),1)
ind_b <- seq(length(variance)+1, length(variance)*2,1)
alpha <- solab(prior_mean,variance)[ind_a]
beta <- solab(prior_mean,variance)[ind_b]
new_mean <- (alpha + tmp$lqt2)/(alpha + beta + tmp$total_carriers)
tmp$p_mean_w <- new_mean
tmp$alpha <- alpha
tmp$beta <- beta
sub.tmp2 <- subset(tmp, !var %in% c(sub.tmp$var, sub.tmp1$var))
tmp <- rbind(sub.tmp,sub.tmp1,sub.tmp2)
#adjust penetrance estimates to same scale with trafficking and peak tail
tmp$p_mean_w <- tmp$p_mean_w*4
View(tmp)
# Selecting specific columns from tmp
tmp_selected <- tmp[tmp$var!="K897T", c("var", "blast_pssm", "provean_score", "pamscore", "aasimilaritymat", "revel_score", "cardiacboost", "am_pathogenicity", "lqt2", "unaff", "resnum", "wtAA", "mutAA", "clinvar.description", "gnomad.v4", "clinvar", "peak_tail", "traff_score", "score_SE", "cohort", "total_carriers", "penetrance_lqt2", "weight", "lqt2_dist", "lqt2_dist_weight", "lqt2_penetranceBayesian_initial", "lqt2_penetranceBayesian", "p_mean_w", "prior_mean", "pore")]
# Merging the selected columns from tmp with cohort
first.event<-merge(first_event_data.cohort, tmp_selected, by = "var", all.x = TRUE)
first.event[!is.na(first.event$QTc) & first.event$QTc > 700,"QTc"] <- 700
first.event$QTc.adj <- first.event$QTc/50
first.serious.event<-merge(first_serious_event_data.cohort, tmp_selected, by = "var", all.x = TRUE)
first.serious.event[!is.na(first.serious.event$QTc) & first.serious.event$QTc > 700,"QTc"] <- 700
first.serious.event$QTc.adj <- first.serious.event$QTc/50
cohort.var <- merge(cohort, tmp_selected, by = "var", all.x = TRUE)
cohort.var[!is.na(cohort.var$QTc) & cohort.var$QTc > 700,"QTc"] <- 700
cohort.var$QTc.adj <- cohort.var$QTc/50
cohort.var$QTc.adj.s <- scale(cohort.var$QTc.adj)
first.event$QTc.adj.s <- scale(first.event$QTc.adj)
first.serious.event$QTc.adj.s <- scale(first.serious.event$QTc.adj)
# Assign (un)affected in 'first.event' dataframe, encoded as 'Status
first.event <- first.event %>%
mutate(Status = ifelse(
(Sex == "Male" & QTc > 450 & Event == 1) |
(Sex == "Female" & QTc > 460 & Event == 1) |
QTc > 480, 1, 0),
ReverseStatus = ifelse(Status == 1, 0, 1)
)
# Assuming your dataframe is named 'first.event'
first.event <- first.event %>%
dplyr::group_by(var) %>%
dplyr::mutate(observed.lqt2 = sum(Status == 1) / n()) %>%
dplyr::ungroup()
View(first.event)
tmp<-first.event[first.event$Mut_Type=="missense",]
View(tmp)
tmp<-first.event[first.event$Mut_Type=="missense" & !is.na(first.event$resnum),]
sum(tmp$Event)
median(tmp[tmp$Event==1,"age"])
median(tmp[tmp$Event==1,"Age_at_event_end"])
names(tmp)
median(tmp[tmp$Event==1,"Age_at_event_end"])
tmp[tmp$Event==1,"Age_at_event_end"]
as.numeric(tmp[tmp$Event==1,"Age_at_event_end"])
tmpp<-tmp[tmp$Event==1,"Age_at_event_end"]
View(tmpp)
mean(tmpp)
mean(tmpp$Age_at_event_end)
median(tmpp$Age_at_event_end)
hist(tmp$p_mean_w)
hist(tmp$p_mean_w/4)
hist(tmp$p_mean_w/4, breaks = 5)
hist(tmp$p_mean_w/4, breaks = 10)
